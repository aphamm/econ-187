library(tidyr)
read_csv("bodyPerformance.csv")
library(tidyverse)
read_csv("bodyPerformance.csv")
library(tidyverse)
df1 <- read_csv("bodyPerformance.csv")
df1
read_csv("Train.csv")
df1
df1$class
unique(df1$class)
df <- read_csv("bodyPerformance.csv")
unique(df$class)
df <- read_csv("bodyPerformance.csv")
unique(df$class)
body <- read_csv("bodyPerformance.csv")
head(body)
body <- read_csv("bodyPerformance.csv")
body %>% mutate(gender = factor(ifelse(gender == "M", 1, 0))
body %>% mutate(gender = factor(ifelse(gender == "M", 1, 0)))
str(body)
is.na(body)
sum(is.na(body))
body <- read_csv("bodyPerformance.csv")
body %>% mutate(gender = factor(ifelse(gender == "M", 1, 0)))
sum(is.na(body))
body <- read_csv("bodyPerformance.csv")
body %>% mutate(gender = factor(ifelse(gender == "M", 1, 0)))
sum(is.na(body))
head(body)
body <- read_csv("bodyPerformance.csv")
body <- body %>% mutate(gender = factor(ifelse(gender == "M", 1, 0)))
sum(is.na(body))
head(body)
spec(body)
body
body
body %>%
ggpairs(aes(col = class, fill = class, alpha = 0.5))
library(ggplot2)
body %>%
ggpairs(aes(col = class, fill = class, alpha = 0.5))
library(ggpairs)
library(ggally)
library(GGally)
library(tidyverse)
library(GGally)
body %>%
ggpairs(aes(col = class, fill = class, alpha = 0.5))
plot <- body %>% ggpairs(columns = 2:4, ggplot2::aes(col = class))
plot
plot <- body %>% ggpairs(columns = 2:4, ggplot2::aes(col = class, fill = class))
plot
head(body)
plot <- body %>% ggpairs(columns = c(1:6,12), ggplot2::aes(col = class, fill = class))
plot
body
names(body)
glm.fit <-
glm(class ~ ., data=body, family=multinomial)
glm.fit <-
multinom(class ~ ., data=body)
library(nnet)
glm.fit <-
multinom(class ~ ., data=body)
summary(glm.fit)
library(caret)
library(caret)
install
install.packages("caret")
library(caret)
model <- cv.glmnet(X_train, y_train, alpha = 0.9, family = "multinomial")
fit.control <- trainControl(method = "repeatedcv", number = 5, repeats = 10)
names(Body)
names(body)
set.seed(42)
fit.control <- trainControl(method = "repeatedcv", number = 5, repeats = 10)
fit <- train(class ~ ., data = body, method = "multinom", trControl = fit.control, trace = FALSE)
rm(list = ls())
# importing data
df = read.csv('house_data.csv')
cat("Number of variables:",length(df))
head(df) # check
# convert ID column into row names
# and drop it
rownames(df) <- df$Id
drops <- c('Id')
df <- df[ , !(names(df) %in% drops)]
# check length of our data
cat("Length of data:",length(df$MSSubClass))
# check if our target variable has any missing values
cat("Missing values in our target var:",sum(is.na(df$SalePrice)))
# great, now we can train-test-split
set.seed(42)
train_index = createDataPartition(df$SalePrice, p = .7, list = FALSE)
library(tidyverse)
library(GGally)
library(nnet)
library(caret)
rm(list = ls())
# importing data
df = read.csv('house_data.csv')
cat("Number of variables:",length(df))
head(df) # check
# convert ID column into row names
# and drop it
rownames(df) <- df$Id
drops <- c('Id')
df <- df[ , !(names(df) %in% drops)]
# check length of our data
cat("Length of data:",length(df$MSSubClass))
# check if our target variable has any missing values
cat("Missing values in our target var:",sum(is.na(df$SalePrice)))
# great, now we can train-test-split
set.seed(42)
train_index = createDataPartition(df$SalePrice, p = .7, list = FALSE)
train <- df[train_index,]
test  <- df[-train_index,]
# drop columns with high number of NA values
# define "high" as >20% null values
drops <- c()
# find the columns with a high number of NAs
for (col in names(train)){
if (sum(is.na(df[col])) > 0.2*nrow(df[col])){
drops <- c(drops,col)
}
}
# drop those columns from our data
train <- train[ , !(names(train) %in% drops)]
test <- test[ , !(names(test) %in% drops)]
# find if there are any variables that don't
# give any information i.e. 0 variance
# this would be the case if a column only
# has 1 unique value
cat("In the training set these variables have 0 variance:",names(sapply(lapply(train, unique), length)[sapply(lapply(train, unique), length) == 1]),"\n")
cat("In the test set these variables have 0 variance:",names(sapply(lapply(test, unique), length)[sapply(lapply(test, unique), length) == 1]),
'\n')
# we will drop the utilities variable
drops <- c('Utilities')
train<- train[ , !(names(train) %in% drops)]
test <- test[ , !(names(test) %in% drops)]
# split into train-test sets
drops <- c('SalePrice')
X_train <-train[ , !(names(train) %in% drops)]
y_train <- train$SalePrice
X_test <- test[ , !(names(test) %in% drops)]
y_test <- test$SalePrice
# handling numeric data
# (1) impute > median
# (2) scale
X_train_scaled <- X_train %>% mutate_if(is.numeric,function(x) ifelse(is.na(x),median(x,na.rm=T),x)) %>% mutate_if(is.numeric, function(x) scale(x))
X_test_scaled <- X_test %>% mutate_if(is.numeric,function(x) ifelse(is.na(x),median(x,na.rm=T),x)) %>% mutate_if(is.numeric, function(x) scale(x))
# handling categorical data
# (1) impute with mode
X_train_scaled <- X_train_scaled %>% mutate_if(is.character,function(x) ifelse(is.na(x),mode(x),x))
X_test_scaled <- X_test_scaled %>% mutate_if(is.character,function(x) ifelse(is.na(x),mode(x),x))
# (2) encode data
X_train_scaled <- X_train_scaled %>% mutate_if(is.character,function(x) as.integer(factor(x)))
X_test_scaled <- X_test_scaled %>% mutate_if(is.character,function(x) as.integer(factor(x)))
library(pls)
set.seed(42)
# combine y_train and X_train_scaled
train_scaled <- X_train_scaled
train_scaled['SalePrice'] <- y_train
# combine y_test and X_test_scaled
test_scaled <- X_test_scaled
test_scaled['SalePrice'] <- y_test
# fit principal component analysis regression
pcr.fit <- pcr(SalePrice ~ .,data = train_scaled,validation = "CV")
# plot RMSE vs number of components
validationplot(pcr.fit, val.type = "RMSEP",
legendpos='topright',
main = 'Number of Principal Components needed to
Predict Sale Price')
ncomps = seq(1:20) + 20
ncomp_score <- c()
for (n in ncomps){
pcr.pred <- predict(pcr.fit, X_test_scaled, ncomp = n)
ncomp_score <- c(ncomp_score,sqrt(mean((pcr.pred-y_test)^2)))
}
# table of ncomps and respective test scores
data.frame(ncomps,ncomp_score)
library(car)
# find linearly dependent variables
fit <- lm(SalePrice~.,data = train_scaled)
ld_vars <- attributes(alias(fit)$Complete)$dimnames[[1]]
cat('Linearly dependent variables:',ld_vars)
# eliminate linearly dep variables
train_scaled_reg <- train_scaled[,-which(names(train_scaled) %in% ld_vars)]
test_scaled_reg <- test_scaled[,-which(names(test_scaled) %in% ld_vars)]
# find variables with VIF > 5
fit <- lm(SalePrice~. ,data = train_scaled_reg)
cat("Variables with VIF > 5:",names(vif(fit)[vif(fit) > 5]))
# eliminate variables with VIF > 5
train_scaled_reg <- train_scaled_reg[,-which(names(train_scaled_reg) %in% names(vif(fit)[vif(fit) > 5]))]
test_scaled_reg <-test_scaled_reg[,-which(names(test_scaled_reg) %in% names(vif(fit)[vif(fit) > 5]))]
library(glmnet)
library(Matrix)
model_ridge = cv.glmnet(x = as.matrix(train_scaled_reg[,-which(names(train_scaled_reg) %in% c("SalePrice"))]), y=train_scaled_reg$SalePrice, alpha = 0)
plot(model_ridge$glmnet.fit, "lambda", label=TRUE)
library(vip)
vip(model_ridge, num_features = 30, geom = "point")
# we can utilize cross validation to train our ridge model
# and find the best lambda
train_control <- trainControl(method = "repeatedcv",
number = 5,
repeats = 1,
search = "random",
verboseIter = FALSE)
ridge_model 	<- train(SalePrice ~ .,
data = train_scaled_reg,
metrics = 'RMSE',
method = "ridge",
tuneLength = 25,
trControl = train_control)
# Predict using the testing data
ridge_preds = predict(ridge_model, newdata = X_test_scaled)
# Evaluate performance
postResample(pred = ridge_preds, obs = y_test)
ridge_model 	<- train(SalePrice ~ .,
data = train_scaled_reg,
metrics = 'RMSE',
method = "ridge",
tuneLength = 25,
trControl = train_control)
ridge_model 	<- train(SalePrice ~ .,
data = train_scaled_reg,
metrics = 'RMSE',
method = "ridge",
tuneLength = 25,
trControl = train_control)
# Predict using the testing data
ridge_preds = predict(ridge_model, newdata = X_test_scaled)
# Evaluate performance
postResample(pred = ridge_preds, obs = y_test)
# Perform 10-fold cross-validation to select lambda
lambdas_to_try <- 10^seq(-3, 5, length.out = 100)
# Setting alpha = 1 implements lasso regression
lasso_cv <- cv.glmnet(x = as.matrix(train_scaled_reg[,-which(names(train_scaled_reg) %in% c("SalePrice"))]), y=train_scaled_reg$SalePrice, alpha = 1, lambda = lambdas_to_try,
standardize = TRUE, nfolds = 10)
# Plot cross-validation results
plot(lasso_cv)
# Best cross-validated lambda
lambda_cv <- lasso_cv$lambda.min
# Fit final model
model_lasso <- glmnet(x = as.matrix(train_scaled_reg[,-which(names(train_scaled_reg) %in% c("SalePrice"))]), y=train_scaled_reg$SalePrice, alpha = 1, lambda = lambda_cv, standardize = TRUE)
#Compare variables across lambdas
plot(lasso_cv$glmnet.fit, "lambda", label=TRUE)
library(vip)
vip(model_lasso, num_features = 30, geom = "point")
# we can utilize cross validation to train our lasso model
# and find the best lambda
train_control <- trainControl(method = "repeatedcv",
number = 5,
repeats = 5,
search = "random",
verboseIter = FALSE)
lasso_model 	<- train(SalePrice ~ .,
data = train_scaled_reg,
metrics = 'RMSE',
method = "lasso",
tuneLength = 25,
trControl = train_control)
?train()
lasso_model 	<- train(SalePrice ~ .,
data = train_scaled_reg,
metrics = 'RMSE',
method = "enet",
tuneLength = 25,
trControl = train_control)
lasso_model 	<- train(SalePrice ~ .,
data = train_scaled_reg,
metrics = 'RMSE',
method = "glmnet",
tuneGrid = expand.grid(alpha = 1,
lambda = 1),
tuneLength = 25,
trControl = train_control)
# Predict using the testing data
lasso_preds = predict(lasso_model, newdata = X_test_scaled)
# Evaluate performance
postResample(pred = lasso_preds, obs = y_test)
8.071006e-01
# Evaluate performance
postResample(pred = ridge_preds, obs = y_test)
# plot RMSE vs number of components
?validationplot(pcr.fit, val.type = "RMSEP",
legendpos='topright',
main = 'Number of Principal Components needed to
Predict Sale Price')
library(pls)
set.seed(42)
# combine y_train and X_train_scaled
train_scaled <- X_train_scaled
train_scaled['SalePrice'] <- y_train
# combine y_test and X_test_scaled
test_scaled <- X_test_scaled
test_scaled['SalePrice'] <- y_test
# fit principal component analysis regression
pcr.fit <- pcr(SalePrice ~ .,data = train_scaled,validation = "CV")
# plot RMSE vs number of components
validationplot(pcr.fit, val.type = "RMSEP",
legendpos='topright',
main = 'Number of Principal Components needed to
Predict Sale Price')
# plot of Rsquared vs number of components
validationplot(pcr.fit, val.type = "R2",
legendpos='topright',
main = 'Number of Principal Components needed to
Predict Sale Price')
# plot RMSE vs number of components
validationplot(pcr.fit, val.type = "RMSEP",
legendpos='topright',
main = 'Number of Principal Components needed to minimise RMSE to
Predict Sale Price')
# plot of Rsquared vs number of components
validationplot(pcr.fit, val.type = "R2",
legendpos='topright',
main = 'Principal Components needed to maximise R-squared to
Predict Sale Price')
