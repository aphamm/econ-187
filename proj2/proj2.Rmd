---
title: "ECON 187: Project 2"
author: "Shannan Liu (305172952), Christina Zhang (605325840), Austin Pham (905318112), Zachary Wrubel (205102460)"
date: "5/17/2022"
fontfamily: mathpazo
output:
  pdf_document:
    toc: true
  fig_caption: yes
  highlight: haddock
  number_sections: true
  df_print: paged
fontsize: 10.5pt
editor_options:
chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls(all=TRUE))
library(tidyverse)
library(GGally)
library(caret)
library(ggplot2)
library(car)
library(pls)
library(glmnet)
library(vip)
library(dplyr)
library(leaps)
library(gam)
library(splines)
library(MASS)
library(boot)
library(tree)
library(rpart)
library(rpart.plot)
library(ISLR2)
library(randomForest)
library(gbm)
```

\newpage

## Acquiring Data & Data Preprocessing
```{r}
df = read_csv('https://raw.githubusercontent.com/onlypham/econ-187/main/proj2/grants.csv')

# initially we have 46 variables
print(length(names(df)))
names(df)
```
  
- Our target variable is `Financial Assistance`
- A lot of the variables are permutations of each other or not useful. After inspecting the data, we've identified that the following variables can be removed:
  - 'Grantee Address', Grantee County Name', 'Grantee City', 'Grant Number', Grantee Name', 'Grant Serial Number', 'Project Period Start Date', 'Project Period Start Date Text String', 'Grant Project Period End Date', 'Grantee State Abbreviation', 'Grant Project Period End Date Text', 'Complete County Name','Grant Program Director Name', 'Grant Program Director Phone Number', 'Congressional District Name', 'State and County Federal Information Processing Standard Code', 'Data Warehouse Record Create Date', 'Data Warehouse Record Create Date Text', 'Uniform Data System Grant Program Description', 'Abstract','DUNS Number','Name of U.S. Senator Number One', 'Name of U.S. Senator Number Two', 'HHS Region Number','U.S. Congressional Representative Name', 'Grantee ZIP Code','Unique Entity Identifier', 'State FIPS Code', 'Grant Program Director E-mail', 'Grant Activity Code', 'Grant Program Name'

```{r}
# Drop the variables listed above
drops <- c('Grantee Address', 'Grantee County Name',
'Grantee City', 'Grant Number', 'Grantee Name',
'Grant Serial Number', 'Project Period Start Date', 
'Project Period Start Date Text String', 
'Grant Project Period End Date', 'Grantee State Abbreviation', 
'Grant Project Period End Date Text', 
'Complete County Name','Grant Program Director Name', 
'Grant Program Director Phone Number', 'Congressional District Name', 
'State and County Federal Information Processing Standard Code', 
'Data Warehouse Record Create Date', 'Data Warehouse Record Create Date Text', 
'Uniform Data System Grant Program Description', 'Abstract',
'DUNS Number','Name of U.S. Senator Number One', 
'Name of U.S. Senator Number Two', 'HHS Region Number',
'U.S. Congressional Representative Name', 'Grantee ZIP Code',
'Unique Entity Identifier','State FIPS Code',
'Grant Program Director E-mail','Grant Activity Code',
'Grant Program Name')
df <- df[ , !(names(df) %in% drops)]

#  now we have 15 variables
print(length(names(df)))
names(df)
```

Now, we can deal with any missing values in our target variable, if any.

```{r}
# check length of our data
cat("Length of data:",length(df$`Financial Assistance`),'\n')

# check if our target variable has any missing values
cat("Missing values in our target var:",sum(is.na(df['Financial Assistance'])))
```

Great, there are no missing values in our target. The next step is to split our data into a training and testing set. Then we'll deal with any general NA values in our dataset.

```{r}
# great, now we can train-test-split
set.seed(42)
train_index = createDataPartition(df$`Financial Assistance`, 
                                  p = .7, list = FALSE)
train <- df[train_index,]
test  <- df[-train_index,]

# drop columns with high number of NA values
# define "high" as >20% null values
drops <- c()
# find the columns with a high number of NAs
for (col in names(train)){
  if (sum(is.na(df[col])) > 0.2*nrow(df[col])){
    drops <- c(drops,col)
  }
}
# drop those columns from our data
train <- train[ , !(names(train) %in% drops)]
test <- test[ , !(names(test) %in% drops)]

# find if there are any variables that don't 
# give any information i.e. 0 variance
# this would be the case if a column only 
# has 1 unique value
cat("In the training set these variables have 0 variance:",names(sapply(lapply(train, unique), length)[sapply(lapply(train, unique), length) == 1]),"\n")
```

None of our variables have 0 variance, so we can move on to processing our data. In other words, scaling our numeric data and encoding the categorical data.

We'll also impute any missing values as needed.

```{r}
# data preprocessing
# split into train-test sets
drops <- c('Financial Assistance')
X_train <-train[ , !(names(train) %in% drops)]
y_train <- train$`Financial Assistance`
X_test <- test[ , !(names(test) %in% drops)]
y_test <- test$`Financial Assistance`

# handling numeric data
# (1) impute > median
# (2) scale 
X_train_scaled <- X_train %>% 
  mutate_if(is.numeric,
            function(x) ifelse(is.na(x),
                               median(x,na.rm=T),x)) %>% 
  mutate_if(is.numeric, function(x) scale(x))

X_test_scaled <- X_test %>% 
  mutate_if(is.numeric,
            function(x) ifelse(is.na(x),
                               median(x,na.rm=T),x)) %>% 
  mutate_if(is.numeric, function(x) scale(x))

# handling categorical data
# (1) impute with mode
X_train_scaled <- X_train_scaled %>% 
  mutate_if(is.character,
            function(x) ifelse(is.na(x),
                               mode(x),x))
X_test_scaled <- X_test_scaled %>% 
  mutate_if(is.character,
            function(x) ifelse(is.na(x),
                               mode(x),x))

# dummy vars
dummy <- dummyVars(" ~ .", 
                   data = X_train_scaled)
X_train_scaled <- data.frame(predict(dummy, 
                                     newdata = X_train_scaled))
X_test_scaled <- data.frame(predict(dummy, 
                                    newdata = X_test_scaled))

# putting all the data back together for easier modeling 
train_scaled <- X_train_scaled
train_scaled['Financial Assistance'] <- y_train

test_scaled <- X_test_scaled 
test_scaled['Financial Assistance'] <- y_test

# check to see if the dummy variable creation process created
# any discrepancies in our training and testing data
cat(length(names(train_scaled)), 
    length(names(test_scaled)))
```

There is an extra variable in our training set that isn't in our testing set.

```{r}
cat("Missing Column(s):",names(train_scaled)[
  !names(train_scaled) %in% names(test_scaled)])
```

Since it's a dummy variable column, we can just create a new zero-column in our test set.

```{r}
test_scaled['X.Congressional.District.Number.42'] = 0

# check again just in case
cat("Missing Column(s):",names(train_scaled)[
  !names(train_scaled) %in% names(test_scaled)])
```

Great, our datasets match.

Now that our data is ready, we can begin fitting our models to predict the amount of financial assistance given for healthcare services in underserved populations of the U.S. 

\newpage

## Peicewise Polynomial Regression

As polynomial regressions perform poorly at the tails since they do not extrapolate well, we will first build a piecewise polynomial model. We will start by using our non-categorical variables and creating a polynomial using one predictor at a time.

```{r}
#Cross validate to choose degree of each spline that minimizes RMSE
set.seed(1)
train.control <- trainControl(method = "cv", number = 10)
rmse <- rep(NA, 15)
for (i in seq_along(rmse)) {
  model <- train(`Financial Assistance` ~ bs(X.Award.Year., 4), 
    data = train_scaled,
    method = "lm", trControl = train.control)
  rmse[i] <- model$results[[2]]
}
rmse
which(rmse == min(rmse))
plot(rmse)
```

From cross-validation we find that df=14 minimizes RMSE. We will now build our model using df=14:

```{r}
library(splines)
pp1 <- lm(`Financial Assistance` ~ bs(X.Award.Year., 14), data=train_scaled)
summary(pp1)
```

We will now repeat for the other two non-categorical variables and compare their performance using ANOVA:

```{r}
set.seed(1)
train.control <- trainControl(method = "cv", number = 10)
rmse <- rep(NA, 15)
for (i in seq_along(rmse)) {
  model <- train(`Financial Assistance` ~ bs(X.Geocoding.Artifact.Address.Primary.X.Coordinate., 4), 
    data = train_scaled,
    method = "lm", trControl = train.control)
  rmse[i] <- model$results[[2]]
}
rmse
which(rmse == min(rmse))
plot(rmse)
```

Cross validation reveals df=7 minimizes RMSE. Hence, we will build our second model with df=7:

```{r}
pp2 <- lm(`Financial Assistance` ~ bs(X.Geocoding.Artifact.Address.Primary.X.Coordinate., 14), data=train_scaled)
summary(pp2)
```

And finally our last non-categorical variable:

```{r}
set.seed(1)
train.control <- trainControl(method = "cv", number = 10)
rmse <- rep(NA, 15)
for (i in seq_along(rmse)) {
  model <- train(`Financial Assistance` ~ bs(X.Geocoding.Artifact.Address.Primary.Y.Coordinate., 4), 
    data = train_scaled,
    method = "lm", trControl = train.control)
  rmse[i] <- model$results[[2]]
}
rmse
which(rmse == min(rmse))
plot(rmse)
```

We will now build our final model with df=7:

```{r}
pp3 <- lm(`Financial Assistance` ~ bs(X.Geocoding.Artifact.Address.Primary.Y.Coordinate., 7), data=train_scaled)
summary(pp3)
```

We will now use an ANOVA test to compare our three spline models:

```{r}
anova(pp1,pp2,pp3,test="F")
```

None of them seem to be significant, so we will now construct a piecewise polynomial using all three non-categorical variables:

```{r}
set.seed(1)
train.control <- trainControl(method = "cv", number = 10)
rmse <- rep(NA, 15)
for (i in seq_along(rmse)) {
  model <- train(`Financial Assistance` ~ bs(X.Award.Year.,4) + bs(X.Geocoding.Artifact.Address.Primary.X.Coordinate., 4) + bs(X.Geocoding.Artifact.Address.Primary.Y.Coordinate., 4), 
    data = train_scaled,
    method = "lm", trControl = train.control)
  rmse[i] <- model$results[[2]]
}
rmse
which(rmse == min(rmse))
plot(rmse)
```

Cross validation chooses df=7 for this model as well. 

```{r}
#Construct Model
pp4 <- lm(`Financial Assistance` ~ bs(X.Award.Year., df=7) + bs(X.Geocoding.Artifact.Address.Primary.X.Coordinate., 
    df=7) + bs(X.Geocoding.Artifact.Address.Primary.Y.Coordinate., 
    df=7), data = train_scaled)
summary(pp4)
```

We will now compare this spline combining all three non-categorical variables with the models consisting of univariate splines. 

```{r}
anova(pp1,pp2,pp3,pp4,test="F")
```
 
The model uisng polynomials of all three non-categorical variables performs the best, so we will measure its performance on our testing data now. 

```{r}
#Evaluate Model
pred.pp <- predict(pp4, newdata=test_scaled)

# RMSE
cat("RMSE for Piecewise Polynomial:",sqrt(
  mean((pred.pp - y_test)^2)),
  "\n")

# MAE
cat("MAE for Piecewise Polynomial:",mean(
  abs(pred.pp - y_test)),
  "\n")
```

Our piecewise polynomial of choice has an RMSE of $4480281$ and MAE of $1743207$, so this will be our model we compare our next models to. 
 
\newpage

##Smoothing Splines 

To try to improve on the performance of the piecewise polynomial model, we will now try fitting smoothing splines to our non-categorical variables. We will try models with a smoothing spline on each individual variable, as well as all the variables in one model. We will chose the lamda through cross validation.

```{r}
#Smoothing spline using X.Award.Year
smooth.spline1 <- smooth.spline(train_scaled$X.Award.Year., train_scaled$`Financial Assistance`, cv=TRUE)
smooth.spline1$df
```

Cross validation chooses a lamda that leads to df=2. We will now plot this model to asses the fit:

```{r}
#plot
plot(train_scaled$X.Award.Year., train_scaled$`Financial Assistance`, cex = .5, ylim=c(0,40000000), col = "darkgrey")
title("Smoothing Spline")
lines(smooth.spline1, col = "blue", lwd = 2)
legend("topright", legend = c("2 DF"),
    col = c("blue"), lty = 1, lwd = 2, cex = .8)
```

We will now try a smoothing spline using X.Geocoding.Artifact.Address.Primary.X.Coordinate.

```{r}
smooth.spline2 <- smooth.spline(train_scaled$X.Geocoding.Artifact.Address.Primary.X.Coordinate., 
                                train_scaled$`Financial Assistance`, cv=TRUE)
smooth.spline2$df
```

Cross validation chooses a lambda that leads to df=199.5382. We will now plot this model to asses the fit:

```{r}
#plot
plot(train_scaled$X.Geocoding.Artifact.Address.Primary.X.Coordinate., train_scaled$`Financial Assistance`, ylim=c(0,10000000), cex = .5, col = "darkgrey")
title("Smoothing Spline")
lines(smooth.spline2, col = "blue", lwd = 2)
legend("topright", legend = c("199 DF"),
    col = c("blue"), lty = 1, lwd = 2, cex = .8)
```

We will now try a smoothing spline using X.Geocoding.Artifact.Address.Primary.Y.Coordinate.

```{r}
smooth.spline3 <- smooth.spline(train_scaled$X.Geocoding.Artifact.Address.Primary.Y.Coordinate., 
                                train_scaled$`Financial Assistance`, cv=TRUE)
smooth.spline3$df
```

Cross validation chooses a lamda that yeilds df=172.7218. We will now plot this model:

```{r}
#plot
plot(train_scaled$X.Geocoding.Artifact.Address.Primary.Y.Coordinate., train_scaled$`Financial Assistance`, ylim=c(0,10000000), cex = .5, col = "darkgrey")
title("Smoothing Spline")
lines(smooth.spline3, col = "blue", lwd = 2)
legend("topright", legend = c("172 DF"),
    col = c("blue"), lty = 1, lwd = 2, cex = .8)
```

We will now predict our test set using each smoothing spline model and choose the one with the lowest RMSE and MAE:

```{r}
predspline1 <- predict(smooth.spline1, 
                  newdata = test_scaled)
predspline2 <- predict(smooth.spline2, 
                  newdata = test_scaled)
predspline3 <- predict(smooth.spline3, 
                  newdata = test_scaled)


# RMSE
cat("RMSE for Smoothing Spline 1:",sqrt(
  mean((predspline1$y - y_test)^2)),
  "\n")
cat("RMSE for Smoothing Spline 2:",sqrt(
  mean((predspline2$y - y_test)^2)),
  "\n")
cat("RMSE for Smoothing Spline 3:",sqrt(
  mean((predspline3$y - y_test)^2)),
  "\n")

# MAE
cat("MAE for Smoothing Spline 1:",mean(
  abs(predspline1$y - y_test)),
  "\n")
cat("MAE for Smoothing Spline 2:",mean(
  abs(predspline2$y - y_test)),
  "\n")
cat("MAE for Smoothing Spline 3:",mean(
  abs(predspline3$y - y_test)),
  "\n")
```

Smoothing Spline 1 performs the best with an RMSE of $4495408$ and an MAE of $1758857$.
 
\newpage

## GAMs

We will now try to improve on the performance of our smoothing splines by predicting Financial Assistance using GAMS:

```{r}
library(splines)
library(gam)
# get non-categorical columns to make our predictions
n_num <- c()
for (col in names(train_scaled)){
  if (nrow(unique(train_scaled[col])) > 2) {
    n_num <- append(n_num,col)
  }
}

# get list of categorical predictor variables
n_cat <- names(train_scaled[,!(
  names(train_scaled) %in% n_num)]) 

# remove "Financial Assistance", our target, 
# from this list of non-categorical 
# predictor variables
n_num <- n_num[-length(n_num)] 

# Create a GAM formula using natural splines 
# on our non-categorical variables
form1 <- as.formula(                     
  paste("`Financial Assistance`~", 
        paste(paste0("ns(",
                     paste0(n_num),
                     sep=", 4)"), 
              collapse = " + ")))

# check what our formula looks like
form1
```

From this, we can see that there are only 3 non-categorical variables in this dataset. We can use these 3 variables to create our first GAM model. Our second GAM model will then incorporate all the other categorical variables as well.

### GAM 1  
```{r}
# build model
gam1 <- gam(form1, data = train_scaled)

# construct plots of X vs ns(X,4)
par(mfrow = c(1, 3))
plot(gam1, se = TRUE, col = "blue")
```

We observe that there are strong nonlinearities in our numeric variables being captured by the splines, which is great! Our model summary below affirms that all three variables are important. This is especially evidenced in the `Anova for Nonparametric Effects` section of the summary.

```{r}
summary(gam1)
```

Now, let's build our second model

### GAM 2  
In this model, we're also going to utilize all of our categorical variables.

```{r}
form2 <- as.formula(                     
  paste("`Financial Assistance`~", 
        paste(paste(paste0("ns(",
                           paste0(n_num),
                           sep=", 4)"), 
                    collapse = " + "),
              paste0(n_cat,
                     collapse="+"),
              sep = "+")))

gam2 <- gam(form2, data = train_scaled)

# check which model is better
anova(gam1, gam2, test = "F")
```

From the results of the Anova test above, we can see that a GAM including categorical variables is better than a GAM without the categorical data. We can test this information below by examining each model's fit vs residuals plot. We can also compare their prediction values on the test set to see if there are any statistically significant differences between each model's predictions.

### Fit vs Residuals Plots

From the plots below, we can see that our second GAM model has better performance when fitting larger values while our first GAM model performs better when predicting smaller fitted values. Still, the second GAM model appears to fit better overall.

```{r}
library(broom)
df <- augment(gam1)
p1 <- ggplot(df, aes(x = .fitted, 
                     y = .resid)) + 
  geom_point() + 
  geom_smooth(method=lm , 
              color="red", 
              fill="#0000FF", se=TRUE) 
p1 + ggtitle("GAM1 Fit vs Residuals Plot") + 
  xlab("Fitted Values") + 
  ylab("Residuals")

df <- augment(gam2)
p1 <- ggplot(df, aes(x = .fitted, 
                     y = .resid)) + 
  geom_point() + 
  geom_smooth(method=lm , 
              color="red", 
              fill="#0000FF", se=TRUE) 
p1 + ggtitle("GAM2 Fit vs Residuals Plot") + 
  xlab("Fitted Values") + 
  ylab("Residuals")
```

### Model Predictions & Performance
Now we can move onto evaluating the performance of our models using metrics such as RMSE and MAE.

```{r}
preds1 <- predict(gam1, 
                  newdata = test_scaled)
preds2 <- predict(gam2, 
                  newdata = test_scaled)

# RMSE
cat("RMSE for GAM1:",sqrt(
  mean((preds1 - y_test)^2)),
  "\n")
cat("RMSE for GAM2:",sqrt(
  mean((preds2 - y_test)^2)),
  "\n")

# MAE
cat("MAE for GAM1:",mean(
  abs(preds1 - y_test)),
  "\n")
cat("MAE for GAM2:",mean(
  abs(preds2 - y_test)))
```

We can see that our second GAM model that includes all the categorical variables performs the best on our dataset. Thus, this is the best model, and the one we will be comparing with all of our other models.

\newpage 
### Choosing Optimal Model:

In conclusion: our best performing piecewise polynomial had an RMSE of ____ and a MAE of ____. Our best performing smoothing spline had an RMSE of $4495408$ and an MAE of $1758857$. Our best performing GAM had an RMSE of $4364093$ and an MAE of $1644067$. Thus, our model of chouce is a GAM consisiting of natural splines of our three non-categorical variables X.Award.Year., X.Geocoding.Artifact.Address.Primary.X.Coordinate., X.Geocoding.Artifact.Address.Primary.Y.Coordinate., along with the other 179 categorical varibles included in the regression. 

\newpage
## Decision Tree

Based on your fits, identify the best model taking into consideration the bias-variance tradeoff. Make sure to discuss your results (including plots and tables), and to use CV and/or bootstrap to evaluate your models’ performance.

```{r}
wine <- read_csv('https://raw.githubusercontent.com/onlypham/econ-187/main/proj2/wine.csv')
colnames(wine) <- c("fixAcid", "volAcid", "citAcid", "resSugar", "chlorides", "freeSul", "totSul", "density", "pH", "sulphates", "alcohol", "quality")
```

## Regression Tree

Let's first create a training set and fit a tree. Looking at the summary we notice that only three of the 3 out of the 11 variables are used in constructing the tree: alcohol, sulphates, and volAcid. This also seems to be the relative order of importance as we first split based on alcohol content and then sulphate content. 

```{r}
set.seed(1)
train <- sample(1:nrow(wine), nrow(wine)*0.8)
tree.wine <- tree(quality ~ ., wine, subset = train)
summary(tree.wine)
plot(tree.wine)
text(tree.wine, pretty = 0)
```

Using cross validation we wish to see if pruning the tree can improve performance.

```{r}
cv.wine <- cv.tree(tree.wine)
plot(cv.wine$size, cv.wine$dev, type = "b", main = "Tree Size Cross Validation", xlab = "Tree Size", ylab = "Error")
```

 We notice that we get really good performance with even just a tree size of just 5-6. Let's just keep a size of 6 and see what the best pruned tree looks like.

```{r}
prune.wine <- prune.tree(tree.wine, best = 6)
plot(prune.wine)
text(prune.wine, pretty = 0)
```

Using this unpruned tree, we get a MSE of $0.4801827$. The RMSE is $0.6929522$ which means that our model leads to test predictions that on average are within $0.6929522$ of the true wine quality score.

```{r}
yhat <- predict(tree.wine, newdata = wine[-train, ])
wine.test <- wine[-train, ] %>% pull(quality)
plot(yhat, wine.test, xlab = "Predicted", ylab = "Actual", main = "Test Prediction for Unpruned Tree")
abline(0, 1)
mean((yhat - wine.test)^2)
```

Now, let's use the rpart method! Let's first split up the data.

```{r}
set.seed(42)
inTraining <- createDataPartition(wine$quality, p = 0.8, list = FALSE)
training <- wine[inTraining,]
testing  <- wine[-inTraining,]
```
Let's build a full tree and performa  k-fold cross-validation to select the optimal cost complexity (cp). The anova method creates a regression tree. Printing the results, we see that our training set has 1281 observations and at the root node we have an $SSE=834.47930$ and a predicted quality mean of $5.636222$ (this is just the sample mean of our training data).

```{r}
set.seed(42)
regression.tree  <- rpart(formula = quality ~ ., data = training, method = "anova", xval = 10, model = TRUE)
print(regression.tree)
```
Using rpart() we grew the full tree using cross-validation to test the perofmrance of the possible comlexity hyperparameters.

```{r}
rpart.plot(regression.tree, yesno = TRUE)
```
Let's use printcp to decide how to prune the tree. Looking at the ouptut we notice that an $nsplit=0$ corresponds to just the root node in which we get a relative error of 1. xerror is the cross-validated SSE and xstd is the standard error. 

```{r}
printcp(regression.tree)
```

If we want the lowest possible error, we'd prune to the tree with the smallest relative SSE. However, we wish to balance predictive power with simplicty so we will prune to a tree relative the smallest relative SSE. Unsurprisingly, we get the lowest xerror with 11 splits. Let's try to be within two standard deviation of 11 split's xerror. This gives us 4 splits, which is pretty consistent with our other cross validated pruned tree model.

```{r}
index <- which(regression.tree$cptable[,"xerror"] == min(regression.tree$cptable[,"xerror"]))
(threshold <- regression.tree$cptable[index,"xerror"] + 2 * regression.tree$cptable[index,"xstd"])
```
Even the graph shows similiar results.

```{r}
plotcp(regression.tree, upper = "splits")
```
Let's prune the tree real quick.

```{r}
regression.prune <- prune(regression.tree, cp = regression.tree$cptable[regression.tree$cptable[, 2] == 4, "CP"])
rpart.plot(regression.prune, yesno = TRUE)
```

Looking at the variable importance, we see Alcohol, Volatile Acid and Acid are most importance.

```{r}
barplot(regression.prune$variable.importance, main = "Variable Importance", horiz = TRUE, las = 2)
```
With this train test split we get a $RMSE=0.723623$. 

```{r}
preds <- predict(regression.prune, testing, type = "vector")
RMSE(pred = preds, obs = testing$quality)
```

Now let's rebuild the model using caret(). We'll make a 10-fold cross validation split to optimize the hyperparameter CP. 

```{r}
set.seed(42)
trControl <- trainControl(method = "cv", number = 10, savePredictions = "final")
regression.cv1 <- train(quality ~ ., data = training, method = "rpart", tuneLength = 5,  metric = "RMSE", trControl = trControl)
```

We see we get the best RMSE when $cp=0.02393885$ We can then refine our search near this location.

```{r}
print(regression.cv1)
```

We see we get the best RMSE when $cp=0.005$. 

```{r}
searchGrid <- expand.grid(cp = seq(from = 0, to = 0.05, by = 0.005))
regression.cv2 <- train(quality ~ ., data = training, method = "rpart", tuneGrid  = searchGrid,  metric = "RMSE", trControl = trControl)
print(regression.cv2)
```

```{r}
rpart.plot(regression.cv2$finalModel)
```

We get the same approximate variable importance plots.

```{r}
plot(varImp(regression.cv2), main="Variable Importance with Simple Regression")
```

We see using the cross-validated pruned tree, we get a  $RMSE=0.6736749$. 

```{r}
preds.cv <- predict(regression.cv2, testing, type = "raw")
RMSE(pred = preds.cv, obs = testing$quality)
```

## Classification Tree

Now, let's create a classification tree by turning our quality variable into a binary response variable corresponding to Good and Bad split when $quality = 6$. 

```{r}
response <- factor(ifelse(wine$quality <= 6, "Bad", "Good"))
wine.class <- data.frame(wine, response)
wine.class <- wine.class[, -12] # remove quality response
```

Here is our basic tree. We currently have a training error rate of $0.09944$. 

```{r}
tree.wine <- tree(response ~ ., wine.class)
summary(tree.wine)
```

```{r}
plot(tree.wine)
text(tree.wine, pretty = 0)
```

Let's use a basic split. We get correction test predictions around $0.884375$ of the time.

```{r}
set.seed(42)
train <- sample(1:nrow(wine.class), nrow(wine.class)*0.8)
wine.class.test <- wine.class[-train, ]
response.test <- response[-train]
tree.wine <- tree(response ~ ., wine.class, subset = train)
tree.pred <- predict(tree.wine, wine.class.test, type = "class")
table(tree.pred, response.test)
(267 + 16) / 320
```

Now, lets perform cross-validation in order to see the best level of tree complexity. 

```{r}
set.seed(42)
cv.wine.class <- cv.tree(tree.wine, FUN = prune.misclass)
plot(cv.wine.class$size, cv.wine.class$dev, type = "b", main = "MSE")
```

Lets apply the prune function to obtain a seven-node tree.

```{r}
prune.wine.class <- prune.misclass(tree.wine, best = 9)
plot(prune.wine.class)
text(prune.wine.class, pretty = 0)
```

This time we get a $0.85625$ of the test observations correctly classified.

```{r}
tree.pred <- predict(prune.wine.class, wine.class.test, type = "class")
table(tree.pred, response.test)
(258 + 16) / 320
```

With our cross-validated classification tree, we get an optimal $cp=0.02150538$

```{r}
set.seed(42)
trControl <- trainControl(method = "cv", number = 10, savePredictions = "final")
classification.cv1 <- train(response ~ ., data = wine.class, method = "rpart", tuneLength = 5, trControl = trControl)
print(classification.cv1)
```

With this cross validated classification tree, we get an accuracy of $0.9$.

```{r}
preds.cv <- predict(classification.cv1, wine.class.test, type = "raw")
table(preds.cv, response.test)
(265+23)/320
```

## Random Forest 

Now let's create a randomForest. The argument mytry=11 indicates all 11 predictors should be considered for each split of the tree

```{r}
set.seed(123)
train <- sample(1:nrow(wine), nrow(wine) / 2)
wine.test <- wine[-train, "quality"]
bag.wine <- randomForest(quality ~ ., data = wine,
    subset = train, mtry = 11, importance = TRUE)
bag.wine
```

```{r}
yhat.bag <- predict(bag.wine, newdata = wine[-train, ])
plot(yhat.bag, wine.test$quality, xlab = "Predicted", ylab = "Actual", main = "Test Prediction for Unpruned Random Forest")
abline(0, 1)
```
The test set MSE associated with the bagged regression tree is $MSE=0.3548118$

```{r}
mean((yhat.bag - wine.test$quality)^2)
```

Let's change up the number of trees.

```{r}
rf.wine <- randomForest(quality ~ ., data =wine,
    subset = train, mtry = 11, ntree=25)
yhat.rf <- predict(rf.wine, newdata = wine[-train, ])
```
The test set MSE associated with the random forest is $MSE=0.3781385$.

```{r}
mean((yhat.rf - wine.test$quality)^2)
```
Now let's try a smaller value of mtry.

```{r}
rf.wine <- randomForest(quality ~ ., data =wine, subset = train, mtry = 5)
yhat.rf <- predict(rf.wine, newdata = wine[-train, ])
```

The test set MSE associated with random forest is $MSE=0.347583$. The MSE improved. Random forest yields a better MSE than bagging in this case.

```{r}
mean((yhat.rf - wine.test$quality)^2)
```
As we check the importance of the variables and plot the importance, we can see similiar results as before. The plot indicates that across all tress considered in random forest, the percent alcohol content of the wine (alcohol) and wine additive that contribute to SO2 levels (sulfates) are the two most important variables.

```{r}
importance(rf.wine)
varImpPlot(rf.wine)
```

## Boosting

Now let's perform boosting. We will use 5000 trees. Setting the interaction.depth to 4 will limit the depth of each tree.

```{r}
set.seed(123)
boost.wine <- gbm(quality ~ ., data = wine[train, ],
    distribution = "gaussian", n.trees = 5000, interaction.depth = 4)
```
As we can from the variable importance, alcohol and volatile acidity are by far the most important variables.

```{r}
summary(boost.wine, las = 2, main = "Variable Importance")
```
Now, let's look at the partial dependence plot.

```{r}
par(mfrow=c(1,2)) 
plot(boost.wine, i = "alcohol")
plot(boost.wine, i = "volAcid")
```

Using prediction, we get a $MSE=0.4732347$

```{r}
yhat.boost <- predict(boost.wine, newdata = wine[-train, ], n.trees = 5000)
mean((yhat.boost - wine.test$quality)^2)
```

Now let's try a different shrinking parameter lambda. With this, we get a $MSE=0.426222$. Our shrinkage parameter become $0.03$.

```{r}
boost.wine <- gbm(quality ~ ., data = wine[train, ],
    distribution = "gaussian", n.trees = 5000,
    interaction.depth = 4, shrinkage = 0.03, verbose = F)
yhat.boost <- predict(boost.wine, newdata = wine[-train, ], n.trees = 5000)
mean((yhat.boost - wine.test$quality)^2)
boost.wine$shrinkage
```
In summation, we get an classification accuracy of $0.9$ with our cross-validated classification tree. For our regression analysis, we get the best results for the following models: $RMSE=0.6736749$ for regression tree, $RMSE=0.5895617$ for random forest and $RMSE=0.6528568$ for boosting.
